   0.0 TEL | Telepresence 0.108 launched at Fri Jan 22 16:12:31 2021
   0.0 TEL |   /usr/bin/telepresence --namespace z2 --swap-deployment nami --docker-run --rm -it -v /home/dmoiseenko/code/kon/nami/src:/usr/src/app/src:z nami-dev npm run dev
   0.0 TEL | uname: uname_result(system='Linux', node='localhost.localdomain', release='5.10.8-100.fc32.x86_64', version='#1 SMP Sun Jan 17 19:52:43 UTC 2021', machine='x86_64')
   0.0 TEL | Platform: linux
   0.0 TEL | WSL: False
   0.0 TEL | Python 3.9.1 (default, Jan 11 2021, 06:37:47)
   0.0 TEL | [GCC 5.4.0 20160609]
   0.0 TEL | BEGIN SPAN main.py:40(main)
   0.0 TEL | BEGIN SPAN startup.py:83(set_kube_command)
   0.0 TEL | Found kubectl -> /home/linuxbrew/.linuxbrew/bin/kubectl
   0.0 TEL | [1] Capturing: kubectl config current-context
   0.0 TEL | [1] captured in 0.03 secs.
   0.0 TEL | [2] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon version --short
   0.5 TEL | [2] captured in 0.44 secs.
   0.5 TEL | [3] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon config view -o json
   0.5 TEL | [3] captured in 0.03 secs.
   0.5 TEL | [4] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon api-versions
   1.1 TEL | [4] captured in 0.58 secs.
   1.1 TEL | Command: kubectl 1.20.2
   1.1 TEL | Context: gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon, namespace: z2, version: 1.17.14-gke.400
   1.1 >>> | Warning: kubectl 1.20.2 may not work correctly with cluster version 1.17.14-gke.400 due to the version discrepancy. See https://kubernetes.io/docs/setup/version-skew-policy/ for more information.
   1.1 >>> | 
   1.1 TEL | END SPAN startup.py:83(set_kube_command)    1.1s
   1.1 >>> | Using a Pod instead of a Deployment for the Telepresence proxy. If you experience problems, please file an issue!
   1.1 >>> | Set the environment variable TELEPRESENCE_USE_DEPLOYMENT to any non-empty value to force the old behavior, e.g.,
   1.1 >>> |     env TELEPRESENCE_USE_DEPLOYMENT=1 telepresence --run curl hello
   1.1 >>> | 
   1.1 TEL | [5] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 get deploy nami -o json
   1.6   5 | {
   1.6   5 |     "apiVersion": "apps/v1",
   1.6   5 |     "kind": "Deployment",
   1.6   5 |     "metadata": {
   1.6   5 |         "annotations": {
   1.6   5 |             "deployment.kubernetes.io/revision": "2",
   1.6   5 |             "meta.helm.sh/release-name": "nami",
   1.6   5 |             "meta.helm.sh/release-namespace": "z2"
   1.6   5 |         },
   1.6   5 |         "creationTimestamp": "2021-01-22T12:43:39Z",
   1.6   5 |         "generation": 6,
   1.6   5 |         "labels": {
   1.6   5 |             "app.kubernetes.io/instance": "nami",
   1.6   5 |             "app.kubernetes.io/managed-by": "Helm",
   1.6   5 |             "app.kubernetes.io/name": "nami",
   1.6   5 |             "app.kubernetes.io/version": "5f633ec9aff74db4de48fbab8a2ddd2f6241f2ec",
   1.6   5 |             "helm.sh/chart": "nami-0.0.503683599_5f633ec9aff74db4de48fbab8a2ddd2f6241f2ec"
   1.6   5 |         },
   1.6   5 |         "name": "nami",
   1.6   5 |         "namespace": "z2",
   1.6   5 |         "resourceVersion": "8813753",
   1.6   5 |         "selfLink": "/apis/apps/v1/namespaces/z2/deployments/nami",
   1.6   5 |         "uid": "ea561257-5699-425e-87ee-ae657b02d241"
   1.6   5 |     },
   1.6   5 |     "spec": {
   1.6   5 |         "progressDeadlineSeconds": 600,
   1.6   5 |         "replicas": 1,
   1.6   5 |         "revisionHistoryLimit": 10,
   1.6   5 |         "selector": {
   1.6   5 |             "matchLabels": {
   1.6   5 |                 "app.kubernetes.io/instance": "nami",
   1.6   5 |                 "app.kubernetes.io/name": "nami"
   1.6   5 |             }
   1.6   5 |         },
   1.6   5 |         "strategy": {
   1.6   5 |             "rollingUpdate": {
   1.6   5 |                 "maxSurge": "25%",
   1.6   5 |                 "maxUnavailable": "25%"
   1.6   5 |             },
   1.6   5 |             "type": "RollingUpdate"
   1.6   5 |         },
   1.6   5 |         "template": {
   1.6   5 |             "metadata": {
   1.6   5 |                 "creationTimestamp": null,
   1.6   5 |                 "labels": {
   1.6   5 |                     "app.kubernetes.io/instance": "nami",
   1.6   5 |                     "app.kubernetes.io/name": "nami"
   1.6   5 |                 }
   1.6   5 |             },
   1.6   5 |             "spec": {
   1.6   5 |                 "containers": [
   1.6   5 |                     {
   1.6   5 |                         "image": "us-east1-docker.pkg.dev/prj-sh-kon-registry-4846/kon/nami:5f633ec9aff74db4de48fbab8a2ddd2f6241f2ec",
   1.6   5 |                         "imagePullPolicy": "IfNotPresent",
   1.6   5 |                         "name": "nami",
   1.6   5 |                         "ports": [
   1.6   5 |                             {
   1.6   5 |                                 "containerPort": 3000,
   1.6   5 |                                 "name": "http",
   1.6   5 |                                 "protocol": "TCP"
   1.6   5 |                             }
   1.6   5 |                         ],
   1.6   5 |                         "resources": {},
   1.6   5 |                         "terminationMessagePath": "/dev/termination-log",
   1.6   5 |                         "terminationMessagePolicy": "File"
   1.6   5 |                     }
   1.6   5 |                 ],
   1.6   5 |                 "dnsPolicy": "ClusterFirst",
   1.6   5 |                 "restartPolicy": "Always",
   1.6   5 |                 "schedulerName": "default-scheduler",
   1.6   5 |                 "securityContext": {},
   1.6   5 |                 "serviceAccount": "nami",
   1.6   5 |                 "serviceAccountName": "nami",
   1.6   5 |                 "terminationGracePeriodSeconds": 30
   1.6   5 |             }
   1.6   5 |         }
   1.6   5 |     },
   1.6   5 |     "status": {
   1.6   5 |         "availableReplicas": 1,
   1.6   5 |         "conditions": [
   1.6   5 |             {
   1.6   5 |                 "lastTransitionTime": "2021-01-22T12:43:39Z",
   1.6   5 |                 "lastUpdateTime": "2021-01-22T12:55:51Z",
   1.6   5 |                 "message": "ReplicaSet \"nami-6b64f99d7d\" has successfully progressed.",
   1.6   5 |                 "reason": "NewReplicaSetAvailable",
   1.6   5 |                 "status": "True",
   1.6   5 |                 "type": "Progressing"
   1.6   5 |             },
   1.6   5 |             {
   1.6   5 |                 "lastTransitionTime": "2021-01-22T13:11:54Z",
   1.6   5 |                 "lastUpdateTime": "2021-01-22T13:11:54Z",
   1.6   5 |                 "message": "Deployment has minimum availability.",
   1.6   5 |                 "reason": "MinimumReplicasAvailable",
   1.6   5 |                 "status": "True",
   1.6   5 |                 "type": "Available"
   1.6   5 |             }
   1.6   5 |         ],
   1.6   5 |         "observedGeneration": 6,
   1.6   5 |         "readyReplicas": 1,
   1.6   5 |         "replicas": 1,
   1.6   5 |         "updatedReplicas": 1
   1.6   5 |     }
   1.6   5 | }
   1.6 TEL | [5] captured in 0.47 secs.
   1.6 TEL | Found ssh -> /usr/bin/ssh
   1.6 TEL | [6] Capturing: ssh -V
   1.6 TEL | [6] captured in 0.00 secs.
   1.6 TEL | Found docker -> /usr/bin/docker
   1.6 TEL | [7] Capturing: docker run --rm -v /tmp/tel-56bfcsqj:/tel alpine:3.6 cat /tel/session_id.txt
   1.9   7 | a392e2446a8440b987ea1932aff26b11
   2.0 TEL | [7] captured in 0.42 secs.
   2.0 TEL | Found sudo -> /usr/bin/sudo
   2.0 TEL | [8] Running: sudo -n echo -n
   2.0 TEL | [8] ran in 0.02 secs.
   2.0 TEL | Found sshfs -> /usr/bin/sshfs
   2.0 TEL | Found fusermount -> /usr/bin/fusermount
   2.0 >>> | Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details.
   2.0 TEL | [9] Running: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 get pods telepresence-connectivity-check --ignore-not-found
   2.6 TEL | [9] ran in 0.60 secs.
   3.2 TEL | Scout info: {'latest_version': '0.108', 'application': 'telepresence', 'notices': []}
   3.2 >>> | Starting network proxy to cluster by swapping out Deployment nami with a proxy Pod
   3.2 TEL | [10] Running: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 create -f -
   3.9  10 | pod/nami-a392e2446a8440b987ea1932aff26b11 created
   3.9 TEL | [10] ran in 0.72 secs.
   3.9 TEL | [11] Running: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 scale Deployment nami --replicas=0
   4.5  11 | deployment.apps/nami scaled
   4.5 TEL | [11] ran in 0.59 secs.
   4.5 TEL | BEGIN SPAN remote.py:109(wait_for_pod)
   4.5 TEL | [12] Running: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 wait --for=condition=ready --timeout=60s pod/nami-a392e2446a8440b987ea1932aff26b11
   8.4  12 | pod/nami-a392e2446a8440b987ea1932aff26b11 condition met
   8.4 TEL | [12] ran in 3.96 secs.
   8.4 TEL | [13] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 get pod nami-a392e2446a8440b987ea1932aff26b11 -o json
   8.9 TEL | [13] captured in 0.45 secs.
   8.9 TEL | END SPAN remote.py:109(wait_for_pod)    4.4s
   8.9 TEL | BEGIN SPAN connect.py:37(connect)
   8.9 TEL | [14] Launching kubectl logs: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 logs -f nami-a392e2446a8440b987ea1932aff26b11 --container nami --tail=10
   8.9 TEL | [15] Launching kubectl port-forward: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 port-forward nami-a392e2446a8440b987ea1932aff26b11 38219:8022
   8.9 TEL | [16] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
   8.9 TEL | [16] exit 255 in 0.01 secs.
   9.2 TEL | [17] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
   9.2 TEL | [17] exit 255 in 0.02 secs.
   9.4 TEL | [18] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
   9.4 TEL | [18] exit 255 in 0.02 secs.
   9.5  14 | 2021-01-22T13:12:39+0000 [-] Loading ./forwarder.py...
   9.5  14 | 2021-01-22T13:12:39+0000 [-] /etc/resolv.conf changed, reparsing
   9.5  14 | 2021-01-22T13:12:39+0000 [-] Resolver added ('11.0.32.10', 53) to server list
   9.5  14 | 2021-01-22T13:12:39+0000 [-] SOCKSv5Factory starting on 9050
   9.5  14 | 2021-01-22T13:12:39+0000 [socks.SOCKSv5Factory#info] Starting factory <socks.SOCKSv5Factory object at 0x7fe8639cad68>
   9.5  14 | 2021-01-22T13:12:39+0000 [-] DNSDatagramProtocol starting on 9053
   9.5  14 | 2021-01-22T13:12:39+0000 [-] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7fe8639d3048>
   9.5  14 | 2021-01-22T13:12:39+0000 [-] Loaded.
   9.5  14 | 2021-01-22T13:12:39+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 20.3.0 (/usr/bin/python3.6 3.6.8) starting up.
   9.5  14 | 2021-01-22T13:12:39+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.
   9.7 TEL | [19] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
   9.7 TEL | [19] exit 255 in 0.00 secs.
  10.0 TEL | [20] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
  10.0 TEL | [20] exit 255 in 0.01 secs.
  10.0  15 | Forwarding from 127.0.0.1:38219 -> 8022
  10.0  15 | Forwarding from [::1]:38219 -> 8022
  10.2 TEL | [21] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 /bin/true
  10.2  15 | Handling connection for 38219
  11.3 TEL | [21] ran in 1.13 secs.
  11.3 >>> | Forwarding remote port 3000 to local port 3000.
  11.3 >>> | 
  11.3 TEL | Launching Web server for proxy poll
  11.3 TEL | [22] Launching SSH port forward (socks and proxy poll): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38219 telepresence@127.0.0.1 -L127.0.0.1:41907:127.0.0.1:9050 -R9055:127.0.0.1:37543
  11.3 TEL | END SPAN connect.py:37(connect)    2.5s
  11.3 TEL | BEGIN SPAN remote_env.py:29(get_remote_env)
  11.3 TEL | [23] Capturing: kubectl --context gke_prj-d-kon-app-d802_us-east1-b_gke-d-kon --namespace z2 exec nami-a392e2446a8440b987ea1932aff26b11 --container nami -- python3 podinfo.py
  11.4  15 | Handling connection for 38219
  12.8 TEL | [23] captured in 1.48 secs.
  12.8 TEL | END SPAN remote_env.py:29(get_remote_env)    1.5s
  12.8 TEL | BEGIN SPAN mount.py:30(mount_remote_volumes)
  12.8 TEL | [24] Running: sudo sshfs -p 38219 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -o allow_other telepresence@127.0.0.1:/ /tmp/tel-56bfcsqj/fs
  12.8  15 | Handling connection for 38219
  14.3 TEL | [24] ran in 1.43 secs.
  14.3 TEL | END SPAN mount.py:30(mount_remote_volumes)    1.4s
  14.3 TEL | BEGIN SPAN container.py:160(run_docker_command)
  14.3 TEL | [25] Launching Network container: docker run --publish=127.0.0.1:42513:38022/tcp --hostname=nami-a392e2446a8440b987ea1932aff26b11 --dns=11.0.32.10 --dns-search=z2.svc.cluster.local --dns-search=svc.cluster.local --dns-search=cluster.local --dns-search=us-east1-b.c.prj-d-kon-app-d802.internal --dns-search=c.prj-d-kon-app-d802.internal --dns-search=google.internal --dns-opt=ndots:5 --rm --privileged --name=telepresence-1611321166-1752136-216632 datawire/telepresence-local:0.108 proxy '{"cidrs": ["0/0"], "expose_ports": [[3000, 3000]], "to_pod": [], "from_pod": []}'
  14.3 TEL | [26] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 42513 root@127.0.0.1 /bin/true
  14.3 TEL | [26] exit 255 in 0.02 secs.
  14.5 TEL | [27] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 42513 root@127.0.0.1 /bin/true
  14.7  25 | [INFO  tini (1)] Spawned child process 'python3' with pid '6'
  14.8  25 |    0.0 TEL | Telepresence 0+unknown launched at Fri Jan 22 13:12:46 2021
  14.8  25 |    0.0 TEL |   /usr/bin/entrypoint.py proxy '{"cidrs": ["0/0"], "expose_ports": [[3000, 3000]], "to_pod": [], "from_pod": []}'
  14.8  25 |    0.0 TEL | uname: uname_result(system='Linux', node='nami-a392e2446a8440b987ea1932aff26b11', release='5.10.8-100.fc32.x86_64', version='#1 SMP Sun Jan 17 19:52:43 UTC 2021', machine='x86_64', processor='')
  14.8  25 |    0.0 TEL | Platform: linux
  14.8  25 |    0.0 TEL | WSL: False
  14.8  25 |    0.0 TEL | Python 3.6.8 (default, Apr 22 2019, 10:28:12)
  14.8  25 |    0.0 TEL | [GCC 6.3.0]
  14.8  25 |    0.0 TEL | [1] Running: /usr/sbin/sshd -e
  14.8  25 |    0.0 TEL | [1] ran in 0.00 secs.
  14.8  25 |    0.0 TEL | [2] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 /bin/true
  14.8  25 |    0.0 TEL | [2] exit 255 in 0.00 secs.
  15.0  25 |    0.3 TEL | [3] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 /bin/true
  15.1  25 |    0.3 TEL | [3] exit 255 in 0.01 secs.
  15.3  25 |    0.5 TEL | [4] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 /bin/true
  15.3  25 |    0.5 TEL | [4] exit 255 in 0.00 secs.
  15.6  25 |    0.8 TEL | [5] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 /bin/true
  15.6  25 |    0.8 TEL | [5] exit 255 in 0.01 secs.
  15.7 TEL | [27] ran in 1.17 secs.
  15.7 TEL | [28] Launching Local SSH port forward: ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 42513 root@127.0.0.1 -R 38023:127.0.0.1:38219
  15.7 TEL | [29] Running: docker run --network=container:telepresence-1611321166-1752136-216632 --rm datawire/telepresence-local:0.108 wait
  15.8  25 |    1.0 TEL | [6] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 /bin/true
  15.8  15 | Handling connection for 38219
  15.9  29 | [INFO  tini (1)] Spawned child process 'python3' with pid '6'
  16.9  25 |    2.2 TEL | [6] ran in 1.12 secs.
  16.9  25 |    2.2 TEL | [7] Capturing: netstat -n
  16.9  25 |    2.2 TEL | [7] captured in 0.00 secs.
  16.9  25 |    2.2 TEL | [8] Launching SSH port forward (exposed ports): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 38023 telepresence@127.0.0.1 -R '*:3000:127.0.0.1:3000'
  16.9  25 |    2.2 TEL | Everything launched. Waiting to exit...
  16.9  15 | Handling connection for 38219
  16.9  25 |    2.2 TEL | BEGIN SPAN runner.py:729(wait_for_exit)
  17.1  25 | Starting sshuttle proxy.
  17.2  25 | firewall manager: Starting firewall with Python version 3.6.8
  17.2  25 | firewall manager: ready method name nat.
  17.2  25 | IPv6 enabled: False
  17.2  25 | UDP enabled: False
  17.2  25 | DNS enabled: True
  17.2  25 | TCP redirector listening on ('127.0.0.1', 12300).
  17.2  25 | DNS listening on ('127.0.0.1', 12300).
  17.2  25 | Starting client with Python version 3.6.8
  17.2  25 | c : connecting to server...
  17.2  15 | Handling connection for 38219
  17.7  25 | Warning: Permanently added '[127.0.0.1]:38023' (ECDSA) to the list of known hosts.
  18.5  25 | Starting server with Python version 3.6.8
  18.5  25 |  s: latency control setting = True
  18.5  25 |  s: available routes:
  18.5  25 |  s:   2/11.4.2.0/24
  18.5  25 | c : Connected.
  18.5  25 | firewall manager: setting up.
  18.5  25 | >> iptables -t nat -N sshuttle-12300
  18.5  25 | >> iptables -t nat -F sshuttle-12300
  18.5  25 | >> iptables -t nat -I OUTPUT 1 -j sshuttle-12300
  18.6  25 | >> iptables -t nat -I PREROUTING 1 -j sshuttle-12300
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 172.17.0.2/32 -p tcp
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 172.17.0.1/32 -p tcp
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 127.0.0.1/32 -p tcp
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 0.0.0.0/0 -p tcp --to-ports 12300 -m ttl ! --ttl 42
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 11.0.32.10/32 -p udp --dport 53 --to-ports 12300 -m ttl ! --ttl 42
  18.6  25 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 224.0.0.252/32 -p udp --dport 5355 --to-ports 12300 -m ttl ! --ttl 42
  18.7  25 | conntrack v1.4.4 (conntrack-tools): 0 flow entries have been deleted.
  21.1  25 | c : DNS request from ('172.17.0.2', 40713) to None: 57 bytes
  21.3  25 | c : DNS request from ('172.17.0.2', 42697) to None: 54 bytes
  22.4  29 | [INFO  tini (1)] Main child exited normally (with status '100')
  22.5 TEL | [29] exit 100 in 6.78 secs.
  22.5 TEL | [30] Capturing: docker run --help
  22.5 TEL | [30] captured in 0.03 secs.
  22.5 TEL | END SPAN container.py:160(run_docker_command)    8.3s
  22.5 >>> | Setup complete. Launching your container.
  22.5 TEL | Everything launched. Waiting to exit...
  22.5 TEL | BEGIN SPAN runner.py:729(wait_for_exit)
  32.0 TEL | [31] Running: sudo -n echo -n
  32.1 TEL | [31] ran in 0.06 secs.
  37.7 TEL | (proxy checking local liveness)
  37.8  14 | 2021-01-22T13:13:09+0000 [Poll#info] Checkpoint
  62.1 TEL | [32] Running: sudo -n echo -n
  62.2 TEL | [32] ran in 0.05 secs.
  67.7 TEL | (proxy checking local liveness)
  67.8  14 | 2021-01-22T13:13:39+0000 [Poll#info] Checkpoint
  92.2 TEL | [33] Running: sudo -n echo -n
  92.2 TEL | [33] ran in 0.04 secs.
  97.7 TEL | (proxy checking local liveness)
  97.8  14 | 2021-01-22T13:14:09+0000 [Poll#info] Checkpoint
 122.3 TEL | [34] Running: sudo -n echo -n
 122.3 TEL | [34] ran in 0.06 secs.
 127.7 TEL | (proxy checking local liveness)
 127.8  14 | 2021-01-22T13:14:39+0000 [Poll#info] Checkpoint
 152.4 TEL | [35] Running: sudo -n echo -n
 152.4 TEL | [35] ran in 0.06 secs.
 157.7 TEL | (proxy checking local liveness)
 157.8  14 | 2021-01-22T13:15:09+0000 [Poll#info] Checkpoint
 182.5 TEL | [36] Running: sudo -n echo -n
 182.5 TEL | [36] ran in 0.02 secs.
 187.7 TEL | (proxy checking local liveness)
 187.8  14 | 2021-01-22T13:15:39+0000 [Poll#info] Checkpoint
 212.5 TEL | [37] Running: sudo -n echo -n
 212.5 TEL | [37] ran in 0.05 secs.
 217.7 TEL | (proxy checking local liveness)
 217.8  14 | 2021-01-22T13:16:09+0000 [Poll#info] Checkpoint
 242.6 TEL | [38] Running: sudo -n echo -n
 242.6 TEL | [38] ran in 0.01 secs.
 247.7 TEL | (proxy checking local liveness)
 247.8  14 | 2021-01-22T13:16:39+0000 [Poll#info] Checkpoint
 272.6 TEL | [39] Running: sudo -n echo -n
 272.6 TEL | [39] ran in 0.01 secs.
 277.7 TEL | (proxy checking local liveness)
 277.8  14 | 2021-01-22T13:17:09+0000 [Poll#info] Checkpoint
 302.7 TEL | [40] Running: sudo -n echo -n
 302.7 TEL | [40] ran in 0.02 secs.
 307.7 TEL | (proxy checking local liveness)
 307.8  14 | 2021-01-22T13:17:39+0000 [Poll#info] Checkpoint
 332.7 TEL | [41] Running: sudo -n echo -n
 332.8 TEL | [41] ran in 0.05 secs.
 337.7 TEL | (proxy checking local liveness)
 337.8  14 | 2021-01-22T13:18:09+0000 [Poll#info] Checkpoint
 362.8 TEL | [42] Running: sudo -n echo -n
 362.8 TEL | [42] ran in 0.05 secs.
 367.7 TEL | (proxy checking local liveness)
 367.8  14 | 2021-01-22T13:18:39+0000 [Poll#info] Checkpoint
 392.9 TEL | [43] Running: sudo -n echo -n
 392.9 TEL | [43] ran in 0.05 secs.
 397.7 TEL | (proxy checking local liveness)
 397.8  14 | 2021-01-22T13:19:09+0000 [Poll#info] Checkpoint
 423.0 TEL | [44] Running: sudo -n echo -n
 423.0 TEL | [44] ran in 0.01 secs.
 427.7 TEL | (proxy checking local liveness)
 427.8  14 | 2021-01-22T13:19:39+0000 [Poll#info] Checkpoint
 453.0 TEL | [45] Running: sudo -n echo -n
 453.0 TEL | [45] ran in 0.01 secs.
 457.7 TEL | (proxy checking local liveness)
 457.8  14 | 2021-01-22T13:20:09+0000 [Poll#info] Checkpoint
 483.0 TEL | [46] Running: sudo -n echo -n
 483.0 TEL | [46] ran in 0.01 secs.
 487.7 TEL | (proxy checking local liveness)
 487.8  14 | 2021-01-22T13:20:39+0000 [Poll#info] Checkpoint
 513.1 TEL | [47] Running: sudo -n echo -n
 513.1 TEL | [47] ran in 0.04 secs.
 517.7 TEL | (proxy checking local liveness)
 517.8  14 | 2021-01-22T13:21:09+0000 [Poll#info] Checkpoint
 543.1 TEL | [48] Running: sudo -n echo -n
 543.2 TEL | [48] ran in 0.02 secs.
 547.7 TEL | (proxy checking local liveness)
 547.8  14 | 2021-01-22T13:21:39+0000 [Poll#info] Checkpoint
 573.2 TEL | [49] Running: sudo -n echo -n
 573.2 TEL | [49] ran in 0.01 secs.
 577.7 TEL | (proxy checking local liveness)
 577.8  14 | 2021-01-22T13:22:09+0000 [Poll#info] Checkpoint
